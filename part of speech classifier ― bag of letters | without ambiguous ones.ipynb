{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Verb Prediction ― bag of letters | without ambiguous ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "This notebook transforms the HTB dataset from https://github.com/UniversalDependencies/UD_Hebrew-HTB, into what you need for the verb-prediction assignment. You may equally copy the code into `.py` file and run it directly outside the jupyter environment here (in a command prompt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T23:23:22.398408Z",
     "start_time": "2019-02-15T23:23:22.393870Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook # progress bars\n",
    "import pyconll # library parsing CoNLL-U files\n",
    "from pprint import pprint # slightly nicer printing of data structures\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T08:05:18.965894Z",
     "start_time": "2019-02-11T08:05:18.957234Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Downloading and loading the HTB corpus\n",
    "To future-proof, we standardize on a specific version of the HTB. <br>Those with a keen eye will notice we use here the quick-and-dirty ⚡ way of launching OS commands from directly within the notebook. <br>⚙ Anyway, make sure you have git installed and working on your OS before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T08:52:28.067048Z",
     "start_time": "2019-02-15T08:52:28.057316Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "!git clone https://github.com/UniversalDependencies/UD_Hebrew-HTB\n",
    "!cd UD_Hebrew-HTB && git checkout 82591c955e86222e32531336ff23e36c220b5846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T08:52:30.603540Z",
     "start_time": "2019-02-15T08:52:28.069166Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "conllu_file = 'UD_Hebrew-HTB/he_htb-ud-train.conllu'\n",
    "conllu = pyconll.load_from_file(conllu_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick data exploration\n",
    "lets quantify how many verbs do we have per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T08:52:30.678478Z",
     "start_time": "2019-02-15T08:52:30.606042Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "counts = []\n",
    "for sentence in conllu:\n",
    "    verbs = 0\n",
    "    for token in sentence:\n",
    "        if token.upos == 'VERB':\n",
    "            verbs += 1 #print(token.form)\n",
    "    counts.append(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T08:52:30.986747Z",
     "start_time": "2019-02-15T08:52:30.680394Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "counts = pd.Series(counts)\n",
    "counts.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:02:39.657684Z",
     "start_time": "2019-02-16T00:02:39.394747Z"
    }
   },
   "outputs": [],
   "source": [
    "verbs = {}\n",
    "non_verbs = {}\n",
    "\n",
    "for sentence in conllu:\n",
    "    for token in sentence:\n",
    "        if token.upos == 'VERB':\n",
    "            if token.form in verbs:\n",
    "                verbs.update({token.form : verbs[token.form]+1})\n",
    "            else:\n",
    "                verbs.update({token.form : 0})\n",
    "        else:\n",
    "            if token.form in non_verbs:\n",
    "                non_verbs.update({token.form : non_verbs[token.form]+1})\n",
    "            else:\n",
    "                non_verbs.update({token.form : 0})\n",
    "\n",
    "    \n",
    "print('{:,} unique verbs in training data'.format(len(verbs)))\n",
    "print('{:,} unique non-verbs in training data'.format(len(non_verbs)))\n",
    "\n",
    "ambiguous = set(verbs.keys()) & set(non_verbs.keys())\n",
    "\n",
    "print('{:,} words are ambiguous'.format(len(ambiguous)))\n",
    "print()\n",
    "#print('ambiguous words:\\n' + str(ambiguous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:02.813358Z",
     "start_time": "2019-02-16T00:03:02.793226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing ambiguous words:\n",
      "\n",
      "4,439 unique verbs in training data\n",
      "24,683 unique non-verbs in training data\n"
     ]
    }
   ],
   "source": [
    "verbs     = {k:v for (k,v) in verbs.items() if not k in ambiguous} # this is called a dict comprehension\n",
    "non_verbs = {k:v for (k,v) in non_verbs.items() if not k in ambiguous}\n",
    "\n",
    "print('after removing ambiguous words:\\n')\n",
    "print('{:,} unique verbs in training data'.format(len(verbs)))\n",
    "print('{:,} unique non-verbs in training data'.format(len(non_verbs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:26.678890Z",
     "start_time": "2019-02-16T00:03:26.525518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963e8a4b92134b6098e3cacc632d07a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "characters = dict()\n",
    "\n",
    "lexicon = list(verbs.keys()) + list(non_verbs.keys())\n",
    "\n",
    "for verb in tqdm_notebook(lexicon):\n",
    "    for char in verb:\n",
    "        if char in characters:\n",
    "            characters[char] += 1\n",
    "        else:\n",
    "            characters[char] = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:27.870930Z",
     "start_time": "2019-02-16T00:03:27.864860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the alhpabet size in this corpus is 49\n",
      "alphabet:\n",
      "['!', '\"', '%', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '_', 'א', 'ב', 'ג', 'ד', 'ה', 'ו', 'ז', 'ח', 'ט', 'י', 'ך', 'כ', 'ל', 'ם', 'מ', 'ן', 'נ', 'ס', 'ע', 'ף', 'פ', 'ץ', 'צ', 'ק', 'ר', 'ש', 'ת']\n"
     ]
    }
   ],
   "source": [
    "alphabet = characters.keys()\n",
    "alphabet = sorted(alphabet)\n",
    "print('the alhpabet size in this corpus is {}'.format(len(alphabet)))\n",
    "print('alphabet:\\n' + str(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:29.901558Z",
     "start_time": "2019-02-16T00:03:29.889646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " '%': 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " ',': 5,\n",
       " '-': 6,\n",
       " '.': 7,\n",
       " '0': 8,\n",
       " '1': 9,\n",
       " '2': 10,\n",
       " '3': 11,\n",
       " '4': 12,\n",
       " '5': 13,\n",
       " '6': 14,\n",
       " '7': 15,\n",
       " '8': 16,\n",
       " '9': 17,\n",
       " ':': 18,\n",
       " ';': 19,\n",
       " '?': 20,\n",
       " '_': 21,\n",
       " 'א': 22,\n",
       " 'ב': 23,\n",
       " 'ג': 24,\n",
       " 'ד': 25,\n",
       " 'ה': 26,\n",
       " 'ו': 27,\n",
       " 'ז': 28,\n",
       " 'ח': 29,\n",
       " 'ט': 30,\n",
       " 'י': 31,\n",
       " 'ך': 32,\n",
       " 'כ': 33,\n",
       " 'ל': 34,\n",
       " 'ם': 35,\n",
       " 'מ': 36,\n",
       " 'ן': 37,\n",
       " 'נ': 38,\n",
       " 'ס': 39,\n",
       " 'ע': 40,\n",
       " 'ף': 41,\n",
       " 'פ': 42,\n",
       " 'ץ': 43,\n",
       " 'צ': 44,\n",
       " 'ק': 45,\n",
       " 'ר': 46,\n",
       " 'ש': 47,\n",
       " 'ת': 48}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_index = dict([(letter, idx) for idx, letter in enumerate(alphabet)]) # this is called a list comprehension\n",
    "\n",
    "def vectorize(word):\n",
    "    ''' turn word into a vector '''\n",
    "    \n",
    "    # letter occurences\n",
    "    vec = [0] * len(alphabet)\n",
    "    for char in word:\n",
    "        vec[letter_index[char]] += 1\n",
    "    \n",
    "    # word length\n",
    "    vec.append(len(word))\n",
    "        \n",
    "    return vec\n",
    "\n",
    "letter_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:31.118698Z",
     "start_time": "2019-02-16T00:03:31.109426Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, y, train_proportion=0.8):\n",
    "    ''' split the given data into train and test sets '''\n",
    "\n",
    "    assert len(X) == len(y), 'input data should have exactly one prediction per input'\n",
    "    assert 0 < train_proportion < 1, 'this function requires a proportion between zero and one as its second argument'\n",
    "    \n",
    "    data_indices = set(range(len(X)))\n",
    "    data_count = len(data_indices)\n",
    "\n",
    "    train_indices = set(random.sample(data_indices, int(data_count * train_proportion)))\n",
    "    test_indices  = data_indices - train_indices\n",
    "    \n",
    "    X_train = [X[idx] for idx in train_indices]\n",
    "    X_test  = [X[idx] for idx in test_indices]\n",
    "    \n",
    "    y_train = [y[idx] for idx in train_indices]\n",
    "    y_test  = [y[idx] for idx in test_indices]\n",
    "\n",
    "    assert len(X_train) + len(X_test) == len(X)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:32.295126Z",
     "start_time": "2019-02-16T00:03:32.120609Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_positive = list(map(vectorize, verbs.keys()))\n",
    "X_negative = list(map(vectorize, non_verbs.keys()))\n",
    "\n",
    "y_positive = [1] * len(X_positive)\n",
    "y_negative = [0] * len(X_negative)\n",
    "\n",
    "X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(X_positive, y_positive)\n",
    "X_train_neg, X_test_neg, y_train_neg, y_test_neg = train_test_split(X_negative, y_negative)\n",
    "\n",
    "X_train = X_train_pos + X_train_neg\n",
    "y_train = y_train_pos + y_train_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:33.359933Z",
     "start_time": "2019-02-16T00:03:33.217106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:35.371953Z",
     "start_time": "2019-02-16T00:03:35.362577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': -10.502928266655765,\n",
       " '\"': -10.502928266655765,\n",
       " '%': -10.502928266655765,\n",
       " '(': -10.502928266655765,\n",
       " ')': -10.502928266655765,\n",
       " ',': -10.502928266655765,\n",
       " '-': -10.502928266655765,\n",
       " '.': -10.502928266655765,\n",
       " '0': -10.502928266655765,\n",
       " '1': -10.502928266655765,\n",
       " '2': -10.502928266655765,\n",
       " '3': -10.502928266655765,\n",
       " '4': -10.502928266655765,\n",
       " '5': -10.502928266655765,\n",
       " '6': -10.502928266655765,\n",
       " '7': -10.502928266655765,\n",
       " '8': -10.502928266655765,\n",
       " '9': -10.502928266655765,\n",
       " ':': -10.502928266655765,\n",
       " ';': -10.502928266655765,\n",
       " '?': -10.502928266655765,\n",
       " '_': -10.502928266655765,\n",
       " 'א': -4.589425261017495,\n",
       " 'ב': -4.097699808624923,\n",
       " 'ג': -4.673982649045557,\n",
       " 'ד': -4.443805071073968,\n",
       " 'ה': -3.2395986491789284,\n",
       " 'ו': -2.87343835026177,\n",
       " 'ז': -5.2046109001077285,\n",
       " 'ח': -4.148558225858414,\n",
       " 'ט': -4.900809445776064,\n",
       " 'י': -2.8900972362484083,\n",
       " 'ך': -6.359793540264232,\n",
       " 'כ': -4.547090897190934,\n",
       " 'ל': -3.4347562662677227,\n",
       " 'ם': -4.352325498209486,\n",
       " 'מ': -3.4364612965188073,\n",
       " 'ן': -5.848967916498242,\n",
       " 'נ': -3.698313746593141,\n",
       " 'ס': -4.531666426865303,\n",
       " 'ע': -4.060388100187566,\n",
       " 'ף': -6.240248389614449,\n",
       " 'פ': -4.316719642755271,\n",
       " 'ץ': -6.765258648372397,\n",
       " 'צ': -4.614050308322884,\n",
       " 'ק': -4.207662265216118,\n",
       " 'ר': -3.5501996220308962,\n",
       " 'ש': -3.8748868904762315,\n",
       " 'ת': -3.3175412510753484}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(letter_index.keys(), clf.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:37.937412Z",
     "start_time": "2019-02-16T00:03:37.929445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'ץץץץץץץץץץץץץץץץץץץץץץ'\n",
    "word = '12324983249898234'\n",
    "clf.predict([vectorize(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:39.333287Z",
     "start_time": "2019-02-16T00:03:39.282431Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = X_test_pos + X_test_neg\n",
    "y_test = y_test_pos + y_test_neg\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:47.194594Z",
     "start_time": "2019-02-16T00:03:47.181060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      1.00      0.92      4937\n",
      "          1       0.00      0.00      0.00       888\n",
      "\n",
      "avg / total       0.72      0.85      0.78      5825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T00:03:54.094712Z",
     "start_time": "2019-02-16T00:03:54.075223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4935,    2],\n",
       "       [ 888,    0]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
