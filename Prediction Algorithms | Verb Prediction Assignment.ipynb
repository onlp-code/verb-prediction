{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Prediction Algorithms | Verb Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T06:52:12.198070Z",
     "start_time": "2019-02-17T06:52:12.191109Z"
    }
   },
   "source": [
    "***Please find the assignments to submit following the sample code.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "This notebook contains working code as the basis for your assignment. It fetches and loads the necessary data you will work on during this assignment, and demonstrates all the initial steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Overview\n",
    "\n",
    "In this assignment, you will learn to use classification algorithms ― Naive Bayes and Logistic Regression ― to deduce whether a word is a verb or not. You will leverage these algorithms to classify yet-unseen words into verbs and non-verbs, and work to improve their rate of success. \n",
    "\n",
    "To do so, you will practice the art of feature engineering ― selecting and implementing features that give the algorithm sufficient power to gain good performance. You will practice the use of evaluation metrics, and work iteratively towards arriving at good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Technical Prerequisites\n",
    "+ **Python 3.6.x.** You may install python via the [Anaconda distribution](https://www.anaconda.com/distribution/#download-section) which will also automatically install many useful python libraries, but this is not mandatory. To import a library in a notebook or python file, it must first be _installed_; this is a prerequisite to running this notebook. Anaconda will out-of-the-box install most/all of the libraries which are imported by this notebook. The latest version of Anaconda comes with python 3.7, so you will have to dig for the previous version of Anaconda.\n",
    "<br><Br>\n",
    "+ Python packages used in this notebook should be installed. <br><br>\n",
    "    + To install any missing or helpful python packages with Anaconda, you would typically issue the following command given below. This command assumes that the package you are after has been made publically available on the official Anaconda distribution channel (called conda-forge). The `conda` command is Anaconda's setup tool that comes along with the Anaconda distribution. If you installed Anaconda, you should have this command available in your command prompt.\n",
    "```\n",
    "conda install -c conda-forge <package name>\n",
    "```\n",
    "    + Further setup notes:\n",
    "         + When run, the `conda install` command may take a while to resolve all dependencies.\n",
    "         + Some packages are not available through Anaconda, in which case use the famous pip utility to install the missing package/s into your python environment.\n",
    "         + You are entirely free to install python and the necessary packages in other ways\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Universal Dependencies Treebanks\n",
    "\n",
    "In this assignment you will use treebanks as the input data. Let us discuss what is a treebank.\n",
    "\n",
    "+ A treebank is a linguistic type of corpus that captures the structure of sentences.\n",
    "+ It also captures the category that each word belongs to ― its Part of Speech.\n",
    "+ Typically, a treebank has been created through labour-intensive annotation.\n",
    "\n",
    "Specifically, in this assignment we will use treebanks which follow the modern [Universal Dependencies](https://universaldependencies.org/introduction.html) standard!\n",
    "\n",
    "You will learn much more about these concepts in later stages of this course. For now, we will only use them in a somewhat simplistic way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Getting Started with this notebook\n",
    "+ Follow the cells and their outputs to confirm you understand what they are doing. \n",
    "+ Use it as a learning opportunity about idiomatic python, if you are relatively new to python!\n",
    "+ Make a local copy of the notebook ― <br>and make sure it is running on your machine, with outpus similar to those already appearing in this pre-run copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:22:01.884264Z",
     "start_time": "2019-02-24T03:22:01.429965Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyconll  # library parsing Universal Dependencies files given in CoNLL-U format\n",
    "import random\n",
    "import itertools \n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm_notebook # progress bar\n",
    "from pprint import pprint  # slightly nicer printing of data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T08:05:18.965894Z",
     "start_time": "2019-02-11T08:05:18.957234Z"
    },
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Downloading and loading the HTB corpus\n",
    "To future-proof, we standardize on a specific version of the HTB. <br>Those with a keen eye will notice that in the following we use the quick-and-dirty ⚡ way of running OS commands from directly within the notebook. <br>⚙ Anyway, make sure you have git installed and working on your OS before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T05:16:41.241982Z",
     "start_time": "2019-02-24T05:16:41.158528Z"
    },
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%script false \n",
    "!git clone https://github.com/UniversalDependencies/UD_Hebrew-HTB\n",
    "!cd UD_Hebrew-HTB && git checkout 82591c955e86222e32531336ff23e36c220b5846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T05:16:46.474101Z",
     "start_time": "2019-02-24T05:16:42.178291Z"
    },
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "conllu1 = pyconll.load_from_file('UD_Hebrew-HTB/he_htb-ud-dev.conllu')\n",
    "conllu2 = pyconll.load_from_file('UD_Hebrew-HTB/he_htb-ud-test.conllu')\n",
    "conllu3 = pyconll.load_from_file('UD_Hebrew-HTB/he_htb-ud-train.conllu')\n",
    "\n",
    "conllu = [conllu1, conllu2, conllu3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Quick data exploration\n",
    "Let us quantify some basic properties of the data, such as how many verbs do we have per sentence, to get a basic grasp of the data, before diving into our tasks. This is typically called the Data Exploration phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:30:59.251937Z",
     "start_time": "2019-02-24T03:30:59.248137Z"
    },
    "hidden": true
   },
   "source": [
    "### Verbs v.s. Non Verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### counting the number of verbs per sentence, and making a histogram of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:22:05.123301Z",
     "start_time": "2019-02-24T03:22:05.062184Z"
    },
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "counts = []\n",
    "for sentence in itertools.chain(*conllu): # the asterisk unpacks the array into a function argument list\n",
    "    verbs = 0\n",
    "    for token in sentence:\n",
    "        if token.upos == 'VERB':\n",
    "            verbs += 1 \n",
    "    \n",
    "    counts.append(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T02:57:47.870186Z",
     "start_time": "2019-02-24T02:57:47.866225Z"
    },
    "hidden": true
   },
   "source": [
    "#### a histogram view of the counts\n",
    "here we exemplify use of the [pandas library](https://towardsdatascience.com/pandas-series-a-lightweight-intro-b7963a0d62a2). pandas is _the_ python library for series and tabular data exploration and manipulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:22:05.430456Z",
     "start_time": "2019-02-24T03:22:05.126012Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># of verbs</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentences count\n",
       "# of verbs                 \n",
       "0                       511\n",
       "1                      1673\n",
       "2                      1748\n",
       "3                      1099\n",
       "4                       604\n",
       "5                       321\n",
       "6                       140\n",
       "7                        66\n",
       "8                        26\n",
       "9                        15\n",
       "10                        6\n",
       "11                        3\n",
       "12                        3\n",
       "15                        1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# turning the array we got, into a pandas series\n",
    "counts = pd.Series(counts)\n",
    "\n",
    "# getting the number of sentences that have n verbs for every n\n",
    "hist = counts.value_counts().sort_index().to_frame('sentences count')\n",
    "hist.index.name = '# of verbs'\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:15:58.535662Z",
     "start_time": "2019-02-24T03:15:58.531854Z"
    },
    "hidden": true
   },
   "source": [
    "#### split the words into verbs and non-verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:22:24.357495Z",
     "start_time": "2019-02-24T03:22:24.038588Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "verbs = {}\n",
    "non_verbs = {}\n",
    "\n",
    "for sentence in itertools.chain(*conllu):\n",
    "    for token in sentence:\n",
    "        if token.upos == 'VERB':\n",
    "            if token.form in verbs:\n",
    "                verbs.update({token.form : verbs[token.form]+1})\n",
    "            else:\n",
    "                verbs.update({token.form : 0})\n",
    "        else:\n",
    "            if token.form in non_verbs:\n",
    "                non_verbs.update({token.form : non_verbs[token.form]+1})\n",
    "            else:\n",
    "                non_verbs.update({token.form : 0})\n",
    "\n",
    "\n",
    "# the next line uses the python set union operator\n",
    "ambiguous = set(verbs.keys()) & set(non_verbs.keys()) \n",
    "                \n",
    "# the (right-hand side of the) next couple of lines are dict comprehensions ― \n",
    "# eseentially that's just a one liner syntax for a loop\n",
    "verbs     = {k:v for (k,v) in verbs.items() if not k in ambiguous} \n",
    "non_verbs = {k:v for (k,v) in non_verbs.items() if not k in ambiguous}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:22:25.468542Z",
     "start_time": "2019-02-24T03:22:25.463085Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 4,896 unique verbs in the data\n",
      "there are 27,573 unique non-verbs in the data\n",
      "632 words appear in the data as both a verb and a non-verb\n"
     ]
    }
   ],
   "source": [
    "print('there are {:,} unique verbs in the data'.format(len(verbs)))\n",
    "print('there are {:,} unique non-verbs in the data'.format(len(non_verbs)))\n",
    "print('{:,} words appear in the data as both a verb and a non-verb'.format(len(ambiguous)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:22:38.081224Z",
     "start_time": "2019-02-24T03:22:38.076068Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion of verbs in the overall lexicon: 0.148\n"
     ]
    }
   ],
   "source": [
    "print('proportion of verbs in the overall lexicon: {:.3f}'.format(len(verbs) / (len(verbs) + len(non_verbs) + len(ambiguous))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Character Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### extracting all ngrams (up to length 2) from all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:39:26.987331Z",
     "start_time": "2019-02-24T03:39:26.765772Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5eb1843007419e82cc86e295ed3887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32469), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ngrams = dict()\n",
    "max_ngram_len = 2\n",
    "\n",
    "lexicon = list(verbs.keys()) + list(non_verbs.keys())\n",
    "\n",
    "# for each verb (tqdm is just a nice utility for getting a progress bar for a long loop)\n",
    "for verb in tqdm_notebook(lexicon):  \n",
    "    # per ngram length\n",
    "    for ngram_len in range(1,max_ngram_len): \n",
    "        # sliding window iterate its ngrams\n",
    "        for idx in range(len(verb) - ngram_len + 1):\n",
    "            ngram = verb[idx : idx + 2] \n",
    "\n",
    "            if ngram in ngrams:\n",
    "                ngrams[ngram] += 1\n",
    "            else:\n",
    "                ngrams[ngram] = 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:39:30.989432Z",
     "start_time": "2019-02-24T03:39:30.983547Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ngrams:\n",
      "\n",
      "['!', '\"', '\"א', '\"ב', '\"ג', '\"ד', '\"ה', '\"ו', '\"ז', '\"ח', '\"ט', '\"י', '\"ך', '\"כ', '\"ל', '\"ם', '\"מ', '\"ן', '\"נ', '\"ס', '\"ע', '\"ף', '\"פ', '\"ץ', '\"צ', '\"ק', '\"ר', '\"ש', '\"ת', '%', '(', ')', ',', ',0', ',1', ',2', ',3', ',4', ',5', ',6', ',7', ',8', ',9', '-', '.', '..', '.0', '.1', '.2', '.3', '.4', '.5', '.6', '.7', '.8', '.9', '.א', '.ב', '.ד', '.כ', '.ס', '.פ', '.ר', '0', '0,', '0.', '00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '1', '1,', '1.', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '1פ', '2', '2,', '2.', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '2ח', '2ש', '3', '3,', '3.', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '4,', '4.', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '5,', '5.', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '6,', '6.', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '6א', '7', '7,', '7.', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '8.', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '9,', '9.', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '9:', ':', ':0', ';', '?', '_', '__', '_א', '_ה', '_ש', 'א', 'א\"', 'א.', 'א_', 'אא', 'אב', 'אג', 'אד', 'אה', 'או', 'אז', 'אח', 'אט', 'אי', 'אך', 'אכ', 'אל', 'אם', 'אמ', 'אן', 'אנ', 'אס', 'אע', 'אף', 'אפ', 'אץ', 'אצ', 'אק', 'אר', 'אש', 'את', 'ב', 'ב\"', 'ב0', 'ב1', 'ב2', 'ב3', 'ב4', 'ב5', 'ב6', 'ב7', 'ב8', 'ב9', 'ב_', 'בא', 'בב', 'בג', 'בד', 'בה', 'בו', 'בז', 'בח', 'בט', 'בי', 'בך', 'בכ', 'בל', 'בם', 'במ', 'בן', 'בנ', 'בס', 'בע', 'בפ', 'בץ', 'בצ', 'בק', 'בר', 'בש', 'בת', 'ג', 'ג\"', 'ג.', 'ג_', 'גא', 'גב', 'גג', 'גד', 'גה', 'גו', 'גז', 'גח', 'גט', 'גי', 'גל', 'גם', 'גמ', 'גן', 'גנ', 'גס', 'גע', 'גף', 'גפ', 'גק', 'גר', 'גש', 'גת', 'ד', 'ד\"', 'ד.', 'ד_', 'דא', 'דב', 'דג', 'דד', 'דה', 'דו', 'דז', 'דח', 'דט', 'די', 'דך', 'דכ', 'דל', 'דם', 'דמ', 'דן', 'דנ', 'דס', 'דע', 'דף', 'דפ', 'דצ', 'דק', 'דר', 'דש', 'דת', 'ה', 'ה\"', 'ה0', 'ה1', 'ה2', 'ה3', 'ה4', 'ה5', 'ה6', 'ה7', 'ה8', 'ה9', 'ה_', 'הא', 'הב', 'הג', 'הד', 'הה', 'הו', 'הז', 'הח', 'הט', 'הי', 'הכ', 'הל', 'הם', 'המ', 'הן', 'הנ', 'הס', 'הע', 'הפ', 'הצ', 'הק', 'הר', 'הש', 'הת', 'ו', 'ו\"', 'ו.', 'ו0', 'ו1', 'ו2', 'ו3', 'ו4', 'ו5', 'ו6', 'ו7', 'ו8', 'ו_', 'וא', 'וב', 'וג', 'וד', 'וה', 'וו', 'וז', 'וח', 'וט', 'וי', 'וך', 'וכ', 'ול', 'ום', 'ומ', 'ון', 'ונ', 'וס', 'וע', 'וף', 'ופ', 'וץ', 'וצ', 'וק', 'ור', 'וש', 'ות', 'ז', 'ז\"', 'ז_', 'זא', 'זב', 'זג', 'זד', 'זה', 'זו', 'זז', 'זח', 'זט', 'זי', 'זכ', 'זל', 'זם', 'זמ', 'זן', 'זנ', 'זע', 'זף', 'זפ', 'זצ', 'זק', 'זר', 'זת', 'ח', 'ח\"', 'ח.', 'ח2', 'ח_', 'חא', 'חב', 'חג', 'חד', 'חה', 'חו', 'חז', 'חח', 'חט', 'חי', 'חכ', 'חל', 'חם', 'חמ', 'חן', 'חנ', 'חס', 'חף', 'חפ', 'חץ', 'חצ', 'חק', 'חר', 'חש', 'חת', 'ט', 'ט\"', 'ט.', 'ט_', 'טא', 'טב', 'טג', 'טד', 'טה', 'טו', 'טז', 'טח', 'טט', 'טי', 'טכ', 'טל', 'טם', 'טמ', 'טן', 'טנ', 'טס', 'טע', 'טף', 'טפ', 'טצ', 'טק', 'טר', 'טש', 'טת', 'י', 'י\"', 'י.', 'י5', 'י_', 'יא', 'יב', 'יג', 'יד', 'יה', 'יו', 'יז', 'יח', 'יט', 'יי', 'יך', 'יכ', 'יל', 'ים', 'ימ', 'ין', 'ינ', 'יס', 'יע', 'יף', 'יפ', 'יץ', 'יצ', 'יק', 'יר', 'יש', 'ית', 'ך', 'ך_', 'כ', 'כ\"', 'כ.', 'כ0', 'כ1', 'כ2', 'כ3', 'כ4', 'כ5', 'כ6', 'כ7', 'כ9', 'כא', 'כב', 'כג', 'כד', 'כה', 'כו', 'כז', 'כח', 'כט', 'כי', 'כך', 'ככ', 'כל', 'כם', 'כמ', 'כן', 'כנ', 'כס', 'כע', 'כף', 'כפ', 'כץ', 'כצ', 'כק', 'כר', 'כש', 'כת', 'ל', 'ל\"', 'ל.', 'ל0', 'ל1', 'ל2', 'ל3', 'ל4', 'ל5', 'ל6', 'ל7', 'ל8', 'ל9', 'ל_', 'לא', 'לב', 'לג', 'לד', 'לה', 'לו', 'לז', 'לח', 'לט', 'לי', 'לך', 'לכ', 'לל', 'לם', 'למ', 'לן', 'לנ', 'לס', 'לע', 'לף', 'לפ', 'לץ', 'לצ', 'לק', 'לר', 'לש', 'לת', 'ם', 'ם_', 'מ', 'מ\"', 'מ0', 'מ1', 'מ2', 'מ3', 'מ5', 'מ6', 'מ7', 'מ8', 'מא', 'מב', 'מג', 'מד', 'מה', 'מו', 'מז', 'מח', 'מט', 'מי', 'מך', 'מכ', 'מל', 'מם', 'ממ', 'מן', 'מנ', 'מס', 'מע', 'מף', 'מפ', 'מץ', 'מצ', 'מק', 'מר', 'מש', 'מת', 'ן', 'ן_', 'נ', 'נ\"', 'נ.', 'נא', 'נב', 'נג', 'נד', 'נה', 'נו', 'נז', 'נח', 'נט', 'ני', 'נך', 'נכ', 'נל', 'נם', 'נמ', 'נן', 'ננ', 'נס', 'נע', 'נף', 'נפ', 'נץ', 'נצ', 'נק', 'נר', 'נש', 'נת', 'ס', 'ס\"', 'ס.', 'ס_', 'סא', 'סב', 'סג', 'סד', 'סה', 'סו', 'סח', 'סט', 'סי', 'סך', 'סכ', 'סל', 'סם', 'סמ', 'סן', 'סנ', 'סס', 'סע', 'סף', 'ספ', 'סצ', 'סק', 'סר', 'סת', 'ע', 'ע\"', 'ע_', 'עא', 'עב', 'עג', 'עד', 'עה', 'עו', 'עז', 'עט', 'עי', 'עך', 'עכ', 'על', 'עם', 'עמ', 'ען', 'ענ', 'עס', 'עע', 'עף', 'עפ', 'עץ', 'עצ', 'עק', 'ער', 'עש', 'עת', 'ף', 'ף_', 'פ', 'פ\"', 'פ.', 'פ6', 'פא', 'פב', 'פג', 'פד', 'פה', 'פו', 'פז', 'פח', 'פט', 'פי', 'פך', 'פכ', 'פל', 'פם', 'פמ', 'פן', 'פנ', 'פס', 'פע', 'פף', 'פפ', 'פץ', 'פצ', 'פק', 'פר', 'פש', 'פת', 'ץ', 'ץ_', 'צ\"', 'צ.', 'צא', 'צב', 'צג', 'צד', 'צה', 'צו', 'צז', 'צח', 'צט', 'צי', 'צכ', 'צל', 'צם', 'צמ', 'צן', 'צנ', 'צס', 'צע', 'צף', 'צפ', 'צץ', 'צצ', 'צק', 'צר', 'צת', 'ק', 'ק\"', 'ק.', 'ק_', 'קא', 'קב', 'קג', 'קד', 'קה', 'קו', 'קז', 'קח', 'קט', 'קי', 'קל', 'קם', 'קמ', 'קן', 'קנ', 'קס', 'קע', 'קף', 'קפ', 'קץ', 'קצ', 'קק', 'קר', 'קש', 'קת', 'ר', 'ר\"', 'ר.', 'ר_', 'רא', 'רב', 'רג', 'רד', 'רה', 'רו', 'רז', 'רח', 'רט', 'רי', 'רך', 'רכ', 'רל', 'רם', 'רמ', 'רן', 'רנ', 'רס', 'רע', 'רף', 'רפ', 'רץ', 'רצ', 'רק', 'רר', 'רש', 'רת', 'ש', 'ש\"', 'ש.', 'ש2', 'ש_', 'שא', 'שב', 'שג', 'שד', 'שה', 'שו', 'שז', 'שח', 'שט', 'שי', 'שך', 'שכ', 'של', 'שם', 'שמ', 'שן', 'שנ', 'שס', 'שע', 'שף', 'שפ', 'שצ', 'שק', 'שר', 'שש', 'שת', 'ת', 'ת\"', 'ת.', 'ת2', 'ת_', 'תא', 'תב', 'תג', 'תד', 'תה', 'תו', 'תז', 'תח', 'תט', 'תי', 'תך', 'תכ', 'תל', 'תם', 'תמ', 'תן', 'תנ', 'תס', 'תע', 'תף', 'תפ', 'תצ', 'תק', 'תר', 'תש', 'תת']\n"
     ]
    }
   ],
   "source": [
    "print(\"the ngrams:\\n\")\n",
    "print(str(sorted(ngrams.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:32:23.427037Z",
     "start_time": "2019-02-24T03:32:23.423004Z"
    },
    "hidden": true
   },
   "source": [
    "#### basic counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:39:29.056800Z",
     "start_time": "2019-02-24T03:39:29.047715Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921 ngrams unique extracted from the data\n",
      "873 2-grams\n",
      "48 1-grams\n"
     ]
    }
   ],
   "source": [
    "print('{} ngrams unique extracted from the data'.format(len(ngrams)))\n",
    "_1grams = list(filter(lambda ngram: len(ngram) == 1, ngrams))\n",
    "_2grams = list(filter(lambda ngram: len(ngram) == 2, ngrams))\n",
    "print('{} 2-grams'.format(len(_2grams)))\n",
    "print('{} 1-grams'.format(len(_1grams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:59:22.633730Z",
     "start_time": "2019-02-24T03:59:22.630051Z"
    },
    "hidden": true
   },
   "source": [
    "#### ngram frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:52:25.380928Z",
     "start_time": "2019-02-24T03:52:25.361882Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram frequencies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>מג</th>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>גי</th>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>יע</th>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>עי</th>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ים</th>\n",
       "      <td>3677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2ש</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ש2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2ח</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>נ.</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.ב</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>921 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    occurences\n",
       "מג         181\n",
       "גי         572\n",
       "יע         513\n",
       "עי         704\n",
       "ים        3677\n",
       "..         ...\n",
       "2ש           1\n",
       "ש2           1\n",
       "2ח           1\n",
       "נ.           2\n",
       ".ב           2\n",
       "\n",
       "[921 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_data = pd.DataFrame.from_dict(ngrams, orient='index', columns=['occurences'])\n",
    "\n",
    "print(\"ngram frequencies:\")\n",
    "with pd.option_context(\"display.max_rows\", 10):\n",
    "    display(hist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T03:59:48.171958Z",
     "start_time": "2019-02-24T03:59:48.166821Z"
    },
    "hidden": true
   },
   "source": [
    "#### frequencies histogram\n",
    "trivially showing that ngrams that occur more often are fewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:01:04.414272Z",
     "start_time": "2019-02-24T04:01:04.340213Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "histogram",
         "uid": "ce4183f2-37e8-11e9-a8c3-d8cb8a18b480",
         "x": [
          181,
          572,
          513,
          704,
          3677,
          4901,
          111,
          244,
          454,
          1199,
          692,
          1041,
          789,
          199,
          763,
          126,
          569,
          4828,
          66,
          632,
          263,
          1210,
          467,
          930,
          272,
          420,
          99,
          1703,
          382,
          124,
          999,
          707,
          1824,
          1888,
          651,
          729,
          626,
          995,
          1127,
          1662,
          374,
          71,
          839,
          679,
          517,
          700,
          986,
          255,
          264,
          164,
          267,
          160,
          169,
          1019,
          97,
          187,
          260,
          434,
          125,
          256,
          1124,
          1370,
          24,
          617,
          617,
          526,
          333,
          166,
          301,
          320,
          274,
          500,
          281,
          451,
          276,
          466,
          289,
          355,
          430,
          294,
          683,
          1221,
          1614,
          359,
          289,
          163,
          692,
          555,
          1860,
          344,
          581,
          469,
          1344,
          562,
          423,
          927,
          3973,
          5926,
          228,
          370,
          688,
          329,
          805,
          267,
          132,
          137,
          217,
          164,
          990,
          1532,
          186,
          245,
          14,
          257,
          824,
          462,
          406,
          554,
          2664,
          486,
          947,
          25,
          875,
          201,
          90,
          989,
          378,
          223,
          556,
          118,
          459,
          1095,
          6,
          284,
          246,
          793,
          407,
          172,
          779,
          325,
          1691,
          326,
          487,
          167,
          590,
          929,
          1864,
          236,
          119,
          11,
          251,
          268,
          300,
          176,
          212,
          802,
          387,
          173,
          781,
          183,
          277,
          453,
          122,
          806,
          109,
          167,
          709,
          481,
          417,
          126,
          199,
          398,
          214,
          145,
          299,
          138,
          439,
          216,
          421,
          17,
          333,
          156,
          451,
          136,
          266,
          58,
          50,
          210,
          603,
          20,
          181,
          104,
          152,
          410,
          551,
          867,
          652,
          1738,
          478,
          465,
          229,
          144,
          703,
          631,
          193,
          113,
          383,
          92,
          19,
          114,
          360,
          96,
          355,
          51,
          189,
          86,
          15,
          27,
          6,
          378,
          88,
          269,
          345,
          75,
          8,
          99,
          79,
          13,
          386,
          159,
          648,
          1340,
          288,
          57,
          250,
          136,
          182,
          179,
          329,
          2377,
          608,
          29,
          133,
          46,
          395,
          31,
          103,
          526,
          185,
          368,
          380,
          200,
          127,
          24,
          394,
          46,
          133,
          158,
          159,
          575,
          115,
          70,
          245,
          51,
          203,
          246,
          447,
          141,
          184,
          323,
          223,
          70,
          341,
          446,
          671,
          197,
          220,
          581,
          209,
          63,
          1207,
          71,
          250,
          452,
          99,
          348,
          54,
          14,
          181,
          333,
          65,
          274,
          251,
          194,
          252,
          265,
          279,
          16,
          17,
          207,
          301,
          169,
          188,
          291,
          131,
          35,
          74,
          203,
          95,
          390,
          93,
          121,
          161,
          198,
          41,
          279,
          263,
          66,
          148,
          237,
          53,
          186,
          283,
          43,
          178,
          108,
          93,
          53,
          113,
          140,
          109,
          159,
          295,
          139,
          183,
          40,
          129,
          324,
          33,
          146,
          66,
          102,
          561,
          78,
          174,
          106,
          5,
          238,
          280,
          157,
          191,
          29,
          114,
          53,
          261,
          62,
          1033,
          279,
          93,
          171,
          230,
          237,
          92,
          366,
          151,
          79,
          36,
          199,
          209,
          62,
          236,
          61,
          344,
          177,
          287,
          77,
          106,
          231,
          258,
          195,
          508,
          9,
          42,
          22,
          250,
          67,
          296,
          149,
          100,
          11,
          63,
          57,
          12,
          13,
          107,
          7,
          54,
          153,
          54,
          136,
          300,
          163,
          186,
          8,
          123,
          371,
          29,
          151,
          91,
          31,
          160,
          75,
          104,
          27,
          69,
          232,
          151,
          54,
          900,
          253,
          132,
          35,
          165,
          37,
          129,
          182,
          19,
          55,
          264,
          263,
          41,
          130,
          165,
          14,
          5,
          24,
          100,
          17,
          292,
          31,
          178,
          52,
          56,
          69,
          23,
          28,
          90,
          75,
          126,
          31,
          154,
          162,
          12,
          83,
          84,
          82,
          154,
          28,
          39,
          50,
          18,
          123,
          26,
          13,
          17,
          42,
          169,
          45,
          18,
          37,
          15,
          28,
          15,
          101,
          6,
          136,
          52,
          41,
          1,
          53,
          13,
          114,
          45,
          249,
          19,
          15,
          17,
          84,
          58,
          106,
          44,
          42,
          46,
          14,
          130,
          42,
          49,
          46,
          11,
          30,
          8,
          40,
          32,
          69,
          55,
          27,
          94,
          32,
          18,
          11,
          1,
          38,
          82,
          4,
          351,
          3,
          38,
          112,
          24,
          26,
          102,
          6,
          12,
          1,
          2,
          3,
          14,
          30,
          34,
          66,
          18,
          1,
          2,
          1,
          3,
          26,
          59,
          31,
          11,
          12,
          1,
          1,
          1,
          42,
          852,
          4,
          117,
          265,
          2,
          30,
          163,
          182,
          441,
          29,
          63,
          1,
          20,
          63,
          7,
          15,
          5,
          47,
          4,
          8,
          1,
          28,
          27,
          37,
          74,
          39,
          7,
          46,
          46,
          1,
          57,
          1,
          2,
          30,
          20,
          60,
          35,
          39,
          1,
          24,
          25,
          39,
          45,
          9,
          72,
          138,
          3,
          3,
          8,
          109,
          1,
          20,
          16,
          34,
          34,
          219,
          116,
          3,
          27,
          19,
          58,
          20,
          13,
          22,
          34,
          87,
          3,
          16,
          3,
          13,
          16,
          19,
          22,
          17,
          10,
          8,
          40,
          4,
          8,
          30,
          19,
          26,
          15,
          4,
          3,
          5,
          8,
          56,
          17,
          7,
          51,
          103,
          9,
          92,
          29,
          6,
          10,
          13,
          11,
          11,
          13,
          50,
          9,
          14,
          5,
          25,
          43,
          3,
          9,
          3,
          7,
          9,
          5,
          9,
          20,
          47,
          11,
          8,
          1,
          11,
          10,
          6,
          11,
          7,
          19,
          14,
          8,
          10,
          14,
          27,
          13,
          9,
          41,
          13,
          47,
          9,
          7,
          9,
          7,
          13,
          18,
          7,
          4,
          10,
          14,
          12,
          21,
          16,
          20,
          14,
          10,
          41,
          19,
          1,
          6,
          12,
          11,
          7,
          3,
          10,
          13,
          10,
          3,
          4,
          17,
          28,
          14,
          12,
          8,
          6,
          25,
          4,
          8,
          23,
          33,
          6,
          8,
          6,
          2,
          5,
          12,
          16,
          2,
          11,
          8,
          4,
          7,
          1,
          9,
          1,
          1,
          17,
          2,
          12,
          5,
          5,
          7,
          14,
          7,
          13,
          6,
          51,
          3,
          8,
          13,
          5,
          13,
          8,
          6,
          11,
          11,
          6,
          1,
          11,
          18,
          4,
          15,
          8,
          14,
          25,
          15,
          5,
          3,
          9,
          6,
          1,
          11,
          23,
          15,
          2,
          3,
          9,
          5,
          3,
          6,
          6,
          8,
          10,
          3,
          11,
          5,
          4,
          10,
          6,
          10,
          8,
          10,
          13,
          5,
          4,
          4,
          2,
          13,
          8,
          3,
          1,
          5,
          7,
          4,
          11,
          13,
          10,
          14,
          2,
          11,
          7,
          3,
          6,
          1,
          1,
          6,
          7,
          2,
          2,
          2,
          7,
          13,
          12,
          12,
          6,
          1,
          10,
          20,
          17,
          11,
          3,
          2,
          14,
          9,
          22,
          5,
          25,
          10,
          1,
          10,
          1,
          5,
          4,
          8,
          11,
          38,
          19,
          22,
          2,
          2,
          2,
          8,
          3,
          22,
          12,
          14,
          2,
          4,
          2,
          1,
          2,
          3,
          1,
          1,
          1,
          5,
          11,
          1,
          3,
          3,
          11,
          2,
          2,
          5,
          1,
          3,
          1,
          1,
          2,
          3,
          2,
          2,
          3,
          2,
          4,
          2,
          2,
          2,
          6,
          3,
          1,
          7,
          2,
          6,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          1,
          1,
          1,
          1,
          3,
          1,
          1,
          1,
          1,
          1,
          2,
          2
         ]
        }
       ],
       "layout": {
        "xaxis": {
         "title": "ngram frequency"
        },
        "yaxis": {
         "title": "ngrams with that frequency"
        }
       }
      },
      "text/html": [
       "<div id=\"cb80143c-19a1-4876-9427-18ed38491bbb\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"cb80143c-19a1-4876-9427-18ed38491bbb\", [{\"x\": [181, 572, 513, 704, 3677, 4901, 111, 244, 454, 1199, 692, 1041, 789, 199, 763, 126, 569, 4828, 66, 632, 263, 1210, 467, 930, 272, 420, 99, 1703, 382, 124, 999, 707, 1824, 1888, 651, 729, 626, 995, 1127, 1662, 374, 71, 839, 679, 517, 700, 986, 255, 264, 164, 267, 160, 169, 1019, 97, 187, 260, 434, 125, 256, 1124, 1370, 24, 617, 617, 526, 333, 166, 301, 320, 274, 500, 281, 451, 276, 466, 289, 355, 430, 294, 683, 1221, 1614, 359, 289, 163, 692, 555, 1860, 344, 581, 469, 1344, 562, 423, 927, 3973, 5926, 228, 370, 688, 329, 805, 267, 132, 137, 217, 164, 990, 1532, 186, 245, 14, 257, 824, 462, 406, 554, 2664, 486, 947, 25, 875, 201, 90, 989, 378, 223, 556, 118, 459, 1095, 6, 284, 246, 793, 407, 172, 779, 325, 1691, 326, 487, 167, 590, 929, 1864, 236, 119, 11, 251, 268, 300, 176, 212, 802, 387, 173, 781, 183, 277, 453, 122, 806, 109, 167, 709, 481, 417, 126, 199, 398, 214, 145, 299, 138, 439, 216, 421, 17, 333, 156, 451, 136, 266, 58, 50, 210, 603, 20, 181, 104, 152, 410, 551, 867, 652, 1738, 478, 465, 229, 144, 703, 631, 193, 113, 383, 92, 19, 114, 360, 96, 355, 51, 189, 86, 15, 27, 6, 378, 88, 269, 345, 75, 8, 99, 79, 13, 386, 159, 648, 1340, 288, 57, 250, 136, 182, 179, 329, 2377, 608, 29, 133, 46, 395, 31, 103, 526, 185, 368, 380, 200, 127, 24, 394, 46, 133, 158, 159, 575, 115, 70, 245, 51, 203, 246, 447, 141, 184, 323, 223, 70, 341, 446, 671, 197, 220, 581, 209, 63, 1207, 71, 250, 452, 99, 348, 54, 14, 181, 333, 65, 274, 251, 194, 252, 265, 279, 16, 17, 207, 301, 169, 188, 291, 131, 35, 74, 203, 95, 390, 93, 121, 161, 198, 41, 279, 263, 66, 148, 237, 53, 186, 283, 43, 178, 108, 93, 53, 113, 140, 109, 159, 295, 139, 183, 40, 129, 324, 33, 146, 66, 102, 561, 78, 174, 106, 5, 238, 280, 157, 191, 29, 114, 53, 261, 62, 1033, 279, 93, 171, 230, 237, 92, 366, 151, 79, 36, 199, 209, 62, 236, 61, 344, 177, 287, 77, 106, 231, 258, 195, 508, 9, 42, 22, 250, 67, 296, 149, 100, 11, 63, 57, 12, 13, 107, 7, 54, 153, 54, 136, 300, 163, 186, 8, 123, 371, 29, 151, 91, 31, 160, 75, 104, 27, 69, 232, 151, 54, 900, 253, 132, 35, 165, 37, 129, 182, 19, 55, 264, 263, 41, 130, 165, 14, 5, 24, 100, 17, 292, 31, 178, 52, 56, 69, 23, 28, 90, 75, 126, 31, 154, 162, 12, 83, 84, 82, 154, 28, 39, 50, 18, 123, 26, 13, 17, 42, 169, 45, 18, 37, 15, 28, 15, 101, 6, 136, 52, 41, 1, 53, 13, 114, 45, 249, 19, 15, 17, 84, 58, 106, 44, 42, 46, 14, 130, 42, 49, 46, 11, 30, 8, 40, 32, 69, 55, 27, 94, 32, 18, 11, 1, 38, 82, 4, 351, 3, 38, 112, 24, 26, 102, 6, 12, 1, 2, 3, 14, 30, 34, 66, 18, 1, 2, 1, 3, 26, 59, 31, 11, 12, 1, 1, 1, 42, 852, 4, 117, 265, 2, 30, 163, 182, 441, 29, 63, 1, 20, 63, 7, 15, 5, 47, 4, 8, 1, 28, 27, 37, 74, 39, 7, 46, 46, 1, 57, 1, 2, 30, 20, 60, 35, 39, 1, 24, 25, 39, 45, 9, 72, 138, 3, 3, 8, 109, 1, 20, 16, 34, 34, 219, 116, 3, 27, 19, 58, 20, 13, 22, 34, 87, 3, 16, 3, 13, 16, 19, 22, 17, 10, 8, 40, 4, 8, 30, 19, 26, 15, 4, 3, 5, 8, 56, 17, 7, 51, 103, 9, 92, 29, 6, 10, 13, 11, 11, 13, 50, 9, 14, 5, 25, 43, 3, 9, 3, 7, 9, 5, 9, 20, 47, 11, 8, 1, 11, 10, 6, 11, 7, 19, 14, 8, 10, 14, 27, 13, 9, 41, 13, 47, 9, 7, 9, 7, 13, 18, 7, 4, 10, 14, 12, 21, 16, 20, 14, 10, 41, 19, 1, 6, 12, 11, 7, 3, 10, 13, 10, 3, 4, 17, 28, 14, 12, 8, 6, 25, 4, 8, 23, 33, 6, 8, 6, 2, 5, 12, 16, 2, 11, 8, 4, 7, 1, 9, 1, 1, 17, 2, 12, 5, 5, 7, 14, 7, 13, 6, 51, 3, 8, 13, 5, 13, 8, 6, 11, 11, 6, 1, 11, 18, 4, 15, 8, 14, 25, 15, 5, 3, 9, 6, 1, 11, 23, 15, 2, 3, 9, 5, 3, 6, 6, 8, 10, 3, 11, 5, 4, 10, 6, 10, 8, 10, 13, 5, 4, 4, 2, 13, 8, 3, 1, 5, 7, 4, 11, 13, 10, 14, 2, 11, 7, 3, 6, 1, 1, 6, 7, 2, 2, 2, 7, 13, 12, 12, 6, 1, 10, 20, 17, 11, 3, 2, 14, 9, 22, 5, 25, 10, 1, 10, 1, 5, 4, 8, 11, 38, 19, 22, 2, 2, 2, 8, 3, 22, 12, 14, 2, 4, 2, 1, 2, 3, 1, 1, 1, 5, 11, 1, 3, 3, 11, 2, 2, 5, 1, 3, 1, 1, 2, 3, 2, 2, 3, 2, 4, 2, 2, 2, 6, 3, 1, 7, 2, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 2], \"type\": \"histogram\", \"uid\": \"ce4183f2-37e8-11e9-a8c3-d8cb8a18b480\"}], {\"xaxis\": {\"title\": \"ngram frequency\"}, \"yaxis\": {\"title\": \"ngrams with that frequency\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"cb80143c-19a1-4876-9427-18ed38491bbb\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"cb80143c-19a1-4876-9427-18ed38491bbb\", [{\"x\": [181, 572, 513, 704, 3677, 4901, 111, 244, 454, 1199, 692, 1041, 789, 199, 763, 126, 569, 4828, 66, 632, 263, 1210, 467, 930, 272, 420, 99, 1703, 382, 124, 999, 707, 1824, 1888, 651, 729, 626, 995, 1127, 1662, 374, 71, 839, 679, 517, 700, 986, 255, 264, 164, 267, 160, 169, 1019, 97, 187, 260, 434, 125, 256, 1124, 1370, 24, 617, 617, 526, 333, 166, 301, 320, 274, 500, 281, 451, 276, 466, 289, 355, 430, 294, 683, 1221, 1614, 359, 289, 163, 692, 555, 1860, 344, 581, 469, 1344, 562, 423, 927, 3973, 5926, 228, 370, 688, 329, 805, 267, 132, 137, 217, 164, 990, 1532, 186, 245, 14, 257, 824, 462, 406, 554, 2664, 486, 947, 25, 875, 201, 90, 989, 378, 223, 556, 118, 459, 1095, 6, 284, 246, 793, 407, 172, 779, 325, 1691, 326, 487, 167, 590, 929, 1864, 236, 119, 11, 251, 268, 300, 176, 212, 802, 387, 173, 781, 183, 277, 453, 122, 806, 109, 167, 709, 481, 417, 126, 199, 398, 214, 145, 299, 138, 439, 216, 421, 17, 333, 156, 451, 136, 266, 58, 50, 210, 603, 20, 181, 104, 152, 410, 551, 867, 652, 1738, 478, 465, 229, 144, 703, 631, 193, 113, 383, 92, 19, 114, 360, 96, 355, 51, 189, 86, 15, 27, 6, 378, 88, 269, 345, 75, 8, 99, 79, 13, 386, 159, 648, 1340, 288, 57, 250, 136, 182, 179, 329, 2377, 608, 29, 133, 46, 395, 31, 103, 526, 185, 368, 380, 200, 127, 24, 394, 46, 133, 158, 159, 575, 115, 70, 245, 51, 203, 246, 447, 141, 184, 323, 223, 70, 341, 446, 671, 197, 220, 581, 209, 63, 1207, 71, 250, 452, 99, 348, 54, 14, 181, 333, 65, 274, 251, 194, 252, 265, 279, 16, 17, 207, 301, 169, 188, 291, 131, 35, 74, 203, 95, 390, 93, 121, 161, 198, 41, 279, 263, 66, 148, 237, 53, 186, 283, 43, 178, 108, 93, 53, 113, 140, 109, 159, 295, 139, 183, 40, 129, 324, 33, 146, 66, 102, 561, 78, 174, 106, 5, 238, 280, 157, 191, 29, 114, 53, 261, 62, 1033, 279, 93, 171, 230, 237, 92, 366, 151, 79, 36, 199, 209, 62, 236, 61, 344, 177, 287, 77, 106, 231, 258, 195, 508, 9, 42, 22, 250, 67, 296, 149, 100, 11, 63, 57, 12, 13, 107, 7, 54, 153, 54, 136, 300, 163, 186, 8, 123, 371, 29, 151, 91, 31, 160, 75, 104, 27, 69, 232, 151, 54, 900, 253, 132, 35, 165, 37, 129, 182, 19, 55, 264, 263, 41, 130, 165, 14, 5, 24, 100, 17, 292, 31, 178, 52, 56, 69, 23, 28, 90, 75, 126, 31, 154, 162, 12, 83, 84, 82, 154, 28, 39, 50, 18, 123, 26, 13, 17, 42, 169, 45, 18, 37, 15, 28, 15, 101, 6, 136, 52, 41, 1, 53, 13, 114, 45, 249, 19, 15, 17, 84, 58, 106, 44, 42, 46, 14, 130, 42, 49, 46, 11, 30, 8, 40, 32, 69, 55, 27, 94, 32, 18, 11, 1, 38, 82, 4, 351, 3, 38, 112, 24, 26, 102, 6, 12, 1, 2, 3, 14, 30, 34, 66, 18, 1, 2, 1, 3, 26, 59, 31, 11, 12, 1, 1, 1, 42, 852, 4, 117, 265, 2, 30, 163, 182, 441, 29, 63, 1, 20, 63, 7, 15, 5, 47, 4, 8, 1, 28, 27, 37, 74, 39, 7, 46, 46, 1, 57, 1, 2, 30, 20, 60, 35, 39, 1, 24, 25, 39, 45, 9, 72, 138, 3, 3, 8, 109, 1, 20, 16, 34, 34, 219, 116, 3, 27, 19, 58, 20, 13, 22, 34, 87, 3, 16, 3, 13, 16, 19, 22, 17, 10, 8, 40, 4, 8, 30, 19, 26, 15, 4, 3, 5, 8, 56, 17, 7, 51, 103, 9, 92, 29, 6, 10, 13, 11, 11, 13, 50, 9, 14, 5, 25, 43, 3, 9, 3, 7, 9, 5, 9, 20, 47, 11, 8, 1, 11, 10, 6, 11, 7, 19, 14, 8, 10, 14, 27, 13, 9, 41, 13, 47, 9, 7, 9, 7, 13, 18, 7, 4, 10, 14, 12, 21, 16, 20, 14, 10, 41, 19, 1, 6, 12, 11, 7, 3, 10, 13, 10, 3, 4, 17, 28, 14, 12, 8, 6, 25, 4, 8, 23, 33, 6, 8, 6, 2, 5, 12, 16, 2, 11, 8, 4, 7, 1, 9, 1, 1, 17, 2, 12, 5, 5, 7, 14, 7, 13, 6, 51, 3, 8, 13, 5, 13, 8, 6, 11, 11, 6, 1, 11, 18, 4, 15, 8, 14, 25, 15, 5, 3, 9, 6, 1, 11, 23, 15, 2, 3, 9, 5, 3, 6, 6, 8, 10, 3, 11, 5, 4, 10, 6, 10, 8, 10, 13, 5, 4, 4, 2, 13, 8, 3, 1, 5, 7, 4, 11, 13, 10, 14, 2, 11, 7, 3, 6, 1, 1, 6, 7, 2, 2, 2, 7, 13, 12, 12, 6, 1, 10, 20, 17, 11, 3, 2, 14, 9, 22, 5, 25, 10, 1, 10, 1, 5, 4, 8, 11, 38, 19, 22, 2, 2, 2, 8, 3, 22, 12, 14, 2, 4, 2, 1, 2, 3, 1, 1, 1, 5, 11, 1, 3, 3, 11, 2, 2, 5, 1, 3, 1, 1, 2, 3, 2, 2, 3, 2, 4, 2, 2, 2, 6, 3, 1, 7, 2, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 2], \"type\": \"histogram\", \"uid\": \"ce4183f2-37e8-11e9-a8c3-d8cb8a18b480\"}], {\"xaxis\": {\"title\": \"ngram frequency\"}, \"yaxis\": {\"title\": \"ngrams with that frequency\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some imports and setup for plotting\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from colab import enable_plotly_in_colab\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "enable_plotly_in_colab()\n",
    "\n",
    "data = [go.Histogram(x=hist_data['occurences'])]\n",
    "fig = go.Figure(data=data, layout=go.Layout(\n",
    "        yaxis=dict(title='ngrams with that frequency'), \n",
    "        xaxis=dict(title='ngram frequency')))\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T05:43:44.877408Z",
     "start_time": "2019-02-17T05:43:44.872621Z"
    },
    "hideOutput": true
   },
   "source": [
    "## The Training Pipeline\n",
    "In the following we test whether ngrams are enough for classifying words into verbs v.s. non-verbs. This demonstrates the entire flow of classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Vectorization\n",
    "Vectorization is the turning of inputs into feature vectors; in our case, turning words into feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:09:23.211248Z",
     "start_time": "2019-02-24T04:09:23.201013Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorize(word):\n",
    "    ''' turn the given word into a feature vector '''\n",
    "    \n",
    "    # ngram occurences\n",
    "    vec1 = [0] * len(ngrams)\n",
    "    for idx, ngram in enumerate(ngrams):\n",
    "        if ngram in word:\n",
    "            if vec1[idx]:\n",
    "                vec1[idx] += 1\n",
    "            else:\n",
    "                vec1[idx] = 1\n",
    "            \n",
    "    # ngram occurences as prefix\n",
    "    vec2 = [0] * len(ngrams)\n",
    "    for idx, ngram in enumerate(ngrams):\n",
    "        if word.startswith(ngram):\n",
    "            if vec2[idx]:\n",
    "                vec2[idx] += 1\n",
    "            else:\n",
    "                vec2[idx] = 1\n",
    "\n",
    "\n",
    "    # ngram occurences as suffix\n",
    "    vec3 = [0] * len(ngrams)\n",
    "    for idx, ngram in enumerate(ngrams):\n",
    "        if word.endswith(ngram):\n",
    "            if vec3[idx]:\n",
    "                vec3[idx] += 1\n",
    "            else:\n",
    "                vec3[idx] = 1\n",
    "        \n",
    "    return vec1 + vec2 + vec3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A spec of our feature vector; This is a map mapping each index of our feature vectors to the meaning of the feature stored on that index. It must be kept aligned to the vectorize function above, and one could make a class encompassing both, under the object oriented programming paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:42:59.329407Z",
     "start_time": "2019-02-24T04:42:59.321843Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feature_vector_description = \\\n",
    "    [(ngram, 'occurs') for ngram in enumerate(ngrams)] + \\\n",
    "    [(ngram, 'startswith') for ngram in enumerate(ngrams)] + \\\n",
    "    [(ngram, 'endswith') for ngram in enumerate(ngrams)]\n",
    "\n",
    "def feature_to_description(idx):\n",
    "    return feature_vector_description[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train Test Split\n",
    "We split the the data into a train and a test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:12:12.344379Z",
     "start_time": "2019-02-24T04:12:12.336585Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, y, train_proportion=0.8):\n",
    "    ''' splits the given data into train and test sets '''\n",
    "\n",
    "    assert len(X) == len(y), 'input data should have exactly one prediction per input'\n",
    "    assert 0 < train_proportion < 1, 'this function requires a proportion between zero and one as its second argument'\n",
    "    \n",
    "    data_indices = set(range(len(X)))\n",
    "    data_count = len(data_indices)\n",
    "\n",
    "    train_indices = set(random.sample(data_indices, int(data_count * train_proportion)))\n",
    "    test_indices  = data_indices - train_indices\n",
    "    \n",
    "    X_train = [X[idx] for idx in train_indices]\n",
    "    X_test  = [X[idx] for idx in test_indices]\n",
    "    \n",
    "    y_train = [y[idx] for idx in train_indices]\n",
    "    y_test  = [y[idx] for idx in test_indices]\n",
    "\n",
    "    assert len(X_train) + len(X_test) == len(X)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:16:02.941775Z",
     "start_time": "2019-02-24T04:16:02.935972Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Arranging the data for classifier training input \n",
    "\n",
    "+ We whip our vectorized data into a matrix $X$ of feature vectors. Each row of the matrix then represents an input sample ― in our case a word ― through its feature vector that we have computed. \n",
    "\n",
    "+ We also derive a _vector_ $y$ of the correct class value per input sample; in our case, $0$ representing a non-verb and $1$ representing a verb).\n",
    "\n",
    "Order matters! the semantics are that the $nth$ row of $X$ has the class label given as the $nth$ element of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:28:41.744495Z",
     "start_time": "2019-02-24T04:28:30.231518Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086002f096374cc6bb4c1783e4b4df04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4896), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af3087e3990420e8dddfcb20a4dc80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27573), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_positive = list(map(vectorize, tqdm_notebook(verbs.keys())))\n",
    "X_negative = list(map(vectorize, tqdm_notebook(non_verbs.keys())))\n",
    "\n",
    "y_positive = [1] * len(X_positive)\n",
    "y_negative = [0] * len(X_negative)\n",
    "\n",
    "X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(X_positive, y_positive)\n",
    "X_train_neg, X_test_neg, y_train_neg, y_test_neg = train_test_split(X_negative, y_negative)\n",
    "\n",
    "X_train = X_train_pos + X_train_neg\n",
    "y_train = y_train_pos + y_train_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:29:26.332602Z",
     "start_time": "2019-02-24T04:29:26.328707Z"
    },
    "hidden": true
   },
   "source": [
    "Maybe we want to bias the classifier with sample weights, but for now this code applies uniform sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:29:39.854491Z",
     "start_time": "2019-02-24T04:29:39.849509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos_sample_weight = 1\n",
    "neg_sample_weight = 1\n",
    "sample_weights = ([pos_sample_weight] * len(X_train_pos)) + ([neg_sample_weight] * len(X_train_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:13:34.605554Z",
     "start_time": "2019-02-24T04:13:34.601736Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Fitting the model\n",
    "**This is the model training step (a.k.a the model fitting)**.<br>\n",
    "We use the classification algorithm implementations of [scikit learn](https://scikit-learn.org/stable/), python's default machine learning library. \n",
    "<br>With Naive Bayes, and roughly in general, the more features and data we have, the longer this will take to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:29:51.129085Z",
     "start_time": "2019-02-24T04:29:47.267365Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = MultinomialNB()\n",
    "#model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train, sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### peeking into the feature importances of the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:43:06.280807Z",
     "start_time": "2019-02-24T04:43:06.267352Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-3.282431357960295, ((118, 'ו'), 'occurs')),\n",
       " (-3.351714042519845, ((239, 'י'), 'occurs')),\n",
       " (-3.6525254064134183, ((17, 'ה'), 'occurs')),\n",
       " (-3.6710513160209324, ((97, 'ת'), 'occurs')),\n",
       " (-3.783333428757265, ((531, 'מ'), 'occurs')),\n",
       " (-3.80704326083632, ((53, 'ל'), 'occurs')),\n",
       " (-3.908887164021568, ((39, 'ר'), 'occurs')),\n",
       " (-4.060195587079896, ((734, 'נ'), 'occurs')),\n",
       " (-4.182126285256361, ((531, 'מ'), 'startswith')),\n",
       " (-4.217663019533148, ((57, 'ש'), 'occurs')),\n",
       " (-4.265291068522403, ((118, 'ו'), 'endswith')),\n",
       " (-4.360465756151194, ((71, 'ע'), 'occurs'))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_coefficients = list(zip(model.coef_[0], feature_vector_description))\n",
    "sorted(list(model_coefficients), reverse=True, key=lambda tuple: tuple[0])[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:42:34.921070Z",
     "start_time": "2019-02-24T04:42:34.915195Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### So we have a trained (fitted) model now. How good is it?\n",
    "looking at specific examples can be tempting, but it doesn't scale as well as making a full evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:43:43.937949Z",
     "start_time": "2019-02-24T04:43:43.918253Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('הלבשה', 0),\n",
       " ('התלבשו', 1),\n",
       " ('התלבשה', 1),\n",
       " ('הלבישה', 0),\n",
       " ('התלבש', 1),\n",
       " ('לבש', 1),\n",
       " ('לבוש', 0)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['הלבשה', 'התלבשו', 'התלבשה', 'הלבישה', 'התלבש', 'לבש', 'לבוש']\n",
    "list(zip(words, (model.predict(list(map(vectorize, words))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:44:54.831037Z",
     "start_time": "2019-02-24T04:44:54.827185Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### To really evaluate our model, we will let it predict the labels of our test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T23:33:13.940467Z",
     "start_time": "2019-02-22T23:33:12.839789Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test_pos + X_test_neg\n",
    "y_test = y_test_pos + y_test_neg\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T04:45:58.033318Z",
     "start_time": "2019-02-24T04:45:58.028798Z"
    }
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T23:33:13.980618Z",
     "start_time": "2019-02-22T23:33:13.942582Z"
    }
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T23:33:24.375100Z",
     "start_time": "2019-02-22T23:33:24.360659Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T23:20:22.285661Z",
     "start_time": "2019-02-22T23:20:22.218385Z"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T23:20:22.346351Z",
     "start_time": "2019-02-22T23:20:22.287376Z"
    }
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T01:37:27.972181Z",
     "start_time": "2019-02-16T01:37:27.965686Z"
    }
   },
   "source": [
    "## _3 pts_ | Question 1 \n",
    "To understand how well (or badly) a model is performing and to gauge progress in improving it, it is good to come up with a baseline. Unless there is already a previously known or public benchmark result to compare to, a naive benchmark is a good idea for getting started. Assume you have no benchmark to compare to, and you hence wish to have a baseline. ***suggest and calculate a reasonable even if naive, baseline over the test set***. hint: think of the class distribution in the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _2 pts_ | Question 2 \n",
    "what can you say about the performance obtained here, according to your baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _3 pts_ | Question 3\n",
    "in light of the above, suggest and implement (from scratch) an adjusted accuracy metric, that would better account for the class imbalance inherent in the data. class imbalance is defined as a case where the distribution of classes in the data is far from being uniform.\n",
    "\n",
    "<span style=\"color:red\">Note: you cannot use any ready-made evaluation metric from sklearn or any other library for this question</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _8 pts_ | Question 4\n",
    "suggest and implement better features based on properties of the Hebrew language. implement the features and provide a notebook with your implementation and the resulting performance report showing the performance improvement.\n",
    "\n",
    "<span style=\"color:red\">Note that you are not allowed to use any of the annotations of the original corpus in this exercise.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _4 pts_ | Question 5\n",
    "now alternatively suggest and apply features specifically for English. Provide a notebook with your implementation classifying over the English data of https://github.com/UniversalDependencies/UD_English-EWT, including a performance report.\n",
    "\n",
    "*use the following code cell to download and use the chosen version of this data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T23:20:22.454088Z",
     "start_time": "2019-02-22T23:20:22.347779Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "!git clone https://github.com/UniversalDependencies/UD_English-EWT\n",
    "!cd UD_English-EWT && git checkout 7be629932192bf1ceb35081fb29b8ecb0bd6d767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _30 pts_ | Question 6 \n",
    "_In this question you will implement a classifier that takes account of the context of a word_. This means, you will no longer classify a word, context-less, but rather, you will transition to classifying words as part of the sentence they appear in. It will open the way for bringing in additional features into play in your model, and very hopefully, lead to substantially improved classification performance.\n",
    "\n",
    "+ You may reuse the code of this notebook in any way, or write from scratch.\n",
    "\n",
    "+ You will be graded based upon:\n",
    "\n",
    "   + dilligence and creativity in adding reasonable features\n",
    "   + score on the test set, obtained when run for grading\n",
    "   + reproducibility: does your submitted solution notebook actually run when downloaded for grading\n",
    "   + cleanly structured, mildly self-explanatory or documented code\n",
    "   \n",
    "<span style=\"color:red\">Note that you are not allowed to use any of the annotations of the original corpus in this exercise.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _20 pts_ | Question 7\n",
    "Implement the _Multinomial Naive Bayes_ algorithm itself from scratch. Establish the same performances as obtained with the ready-made Multinomial Naive Bayes algorithm that you have used so far.\n",
    "\n",
    "What to submit for this?\n",
    "+ a working notebook using your own implemented Naive Bayes classifier rather than the sklearn one, per each of the programming assignments you have already implemented above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T06:13:49.246793Z",
     "start_time": "2019-02-17T06:13:49.240278Z"
    }
   },
   "source": [
    "## _20 pts_ | Question 8\n",
    "Implement the _logistic regression_ algorithm itself from scratch. Establish the same performances as obtained with the ready-made logistic regression algorithm that you have used so far.\n",
    "\n",
    "What to submit for this?\n",
    "+ a working notebook using your own implemented logistic regression algorithm rather than the sklearn one, per each of the programming assignments you have already implemented above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _10 pts_ | Question 9\n",
    "*Manual domain adaptation.* \n",
    "<br>\n",
    "1. obtain an unrelated Hebrew dataset; \n",
    "\n",
    "    + this can be a liberated copy of your gmail inbox ([google data takeout](https://takeout.google.com/settings/takeout))\n",
    "    + a whatsapp data export\n",
    "    + public hebrew tweets\n",
    "    + the hebrew wikipedia\n",
    "\n",
    "\n",
    "2. Extract 50 sentences of that data\n",
    "<br><br>\n",
    "3. Use your best accomplished model over that subset of the data.\n",
    "\n",
    "<br><br>\n",
    "What to submit for this?\n",
    "1. The 50 sentences and your annotation of them (only annotate verb / non-verb per word)\n",
    "2. Your evaluation over them (also 'comparing to the original results)\n",
    "3. Now try to analyze toward a qualitative description of the difference in the performance; Submit your analysis.\n",
    "\n",
    "\n",
    "+ _15 pts bonus:_ modify the training data to improve performance over your data. submit your modified training data here as well.\n",
    "\n",
    "<span style=\"color:gray\">what not to include in your submission?</span>\n",
    "+ <span style=\"color:gray\">You may not include in your new data, any data explicitly or implicitly disclosing the identity or personal information of any person/s. Anonymize any non-public data that you use, as necessary to comply with this requirement.</span>\n",
    "+ <span style=\"color:gray\">You may not include any proprietary or confidential data in your new data.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Submission Notes</span>\n",
    "\n",
    "+ <span style=\"color:red\">Note that non-reproducible results will be ignored.</span>\n",
    "+ <span style=\"color:red\">Any submission that does not actually run (halts with an error) will immediately deduce 5 pts. Any re-submission that does not actually run (halts with an error) will deduce additional 5 pts each. So make sure to verify that your code submission runs before submitting it.</span>\n",
    "+ <span style=\"color:gray\">if any of your code should take more than 10 minutes to run, require more than 16GB of memory, or happen to rely on a GPU, please provide a notifcation in advance.\n",
    "\n",
    "The way to avoid this troubles, is to bundle your solution into a compressed zip archive, unzip it to an empty folder, and verify it is running from scratch from there.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
