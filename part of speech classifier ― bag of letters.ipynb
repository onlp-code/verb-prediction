{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Verb Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "This notebook transforms the HTB dataset from https://github.com/UniversalDependencies/UD_Hebrew-HTB, into what you need for the verb-prediction assignment. You may equally copy the code into `.py` file and run it directly outside the jupyter environment here (in a command prompt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T23:23:22.398408Z",
     "start_time": "2019-02-15T23:23:22.393870Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook # progress bars\n",
    "import pyconll # library parsing CoNLL-U files\n",
    "from pprint import pprint # slightly nicer printing of data structures\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T08:05:18.965894Z",
     "start_time": "2019-02-11T08:05:18.957234Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Downloading and loading the HTB corpus\n",
    "To future-proof, we standardize on a specific version of the HTB. <br>Those with a keen eye will notice we use here the quick-and-dirty ⚡ way of launching OS commands from directly within the notebook. <br>⚙ Anyway, make sure you have git installed and working on your OS before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T08:52:28.067048Z",
     "start_time": "2019-02-15T08:52:28.057316Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "!git clone https://github.com/UniversalDependencies/UD_Hebrew-HTB\n",
    "!cd UD_Hebrew-HTB && git checkout 82591c955e86222e32531336ff23e36c220b5846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T08:52:30.603540Z",
     "start_time": "2019-02-15T08:52:28.069166Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "conllu_file = 'UD_Hebrew-HTB/he_htb-ud-train.conllu'\n",
    "conllu = pyconll.load_from_file(conllu_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick data exploration\n",
    "lets quantify how many verbs do we have per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T08:52:30.678478Z",
     "start_time": "2019-02-15T08:52:30.606042Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "counts = []\n",
    "for sentence in conllu:\n",
    "    verbs = 0\n",
    "    for token in sentence:\n",
    "        if token.upos == 'VERB':\n",
    "            verbs += 1 #print(token.form)\n",
    "    counts.append(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T08:52:30.986747Z",
     "start_time": "2019-02-15T08:52:30.680394Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "counts = pd.Series(counts)\n",
    "counts.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T22:55:26.619892Z",
     "start_time": "2019-02-15T22:55:26.315833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,984 unique verbs in training data\n",
      "25,228 unique non-verbs in training data\n",
      "545 words are ambiguous\n",
      "ambiguous words:\n",
      "{'אשמים', 'משנה', 'מוזמן', 'הבהיר', 'פחדה', 'מועדים', 'אוכל', 'חוזה', 'מקומם', 'שותף', 'נוכח', 'חתומות', 'כלל', 'מעורבים', 'חולף', 'קונה', 'מעניין', 'הערים', 'כתבו', 'מרים', 'נדון', 'עוברים', 'סומך', 'מבין', 'חסר', 'הוצאה', 'למתן', 'לנו', 'הבנתי', 'מאמן', 'רודפים', 'יוצאי', 'איים', 'עובדת', 'מרבה', 'לתקן', 'לפנות', 'לגמול', 'מסירות', 'נטל', 'פטר', 'מוליך', 'כלא', 'מתוך', 'מונחים', 'שקועה', 'ידוע', 'לחוקרו', 'מכירה', 'מקבלת', 'מנהל', 'מחלקים', 'הקלה', 'להפכם', 'לדבר', 'מעניינת', 'יוצרים', 'מאמינים', 'להציגה', 'תומכת', 'ניתנים', 'נושא', 'מוכרת', 'נגיד', 'עלה', 'לדעת', 'קדמה', 'ניתנת', 'משך', 'מובנת', 'מכפר', 'שמה', 'קברו', 'כותב', 'פרצה', 'יציע', 'מראיין', 'נבחרים', 'כעס', 'שכבה', 'מזכירות', 'טענה', 'מושלמת', 'שורר', 'נתונים', 'חיים', 'לחצה', 'מספר', 'גורמים', 'דומה', 'לרכוש', 'באות', 'מחייבים', 'מתמחים', 'מצויים', 'לקח', 'לבחור', 'מובילים', 'גאה', 'כרוך', 'עודד', 'נתן', 'מבקרים', 'מגן', 'מודד', 'מעוות', 'תלויה', 'טרוד', 'שקול', 'מבוססים', 'באים', 'צבועים', 'דן', 'שלט', 'עובדים', 'מוכרים', 'אהבה', 'מראות', 'צר', 'מטיילים', 'להציתו', 'עורך', 'מבוססת', 'נוספים', 'הופיעו', 'להצילם', 'מינה', 'שפר', 'בולטת', 'עוסקים', 'חוקרת', 'היה', 'להביאו', 'מונע', 'נאסר', 'ניכרת', 'הלך', 'רץ', 'שעה', 'חי', 'תובעת', 'מתאים', 'סימן', 'חלק', 'רואה', 'שיגרה', 'פתוח', 'מרוכז', 'אין', 'נבחרת', 'רבים', 'מאוחדות', 'שינה', 'מחברת', 'עזר', 'ערך', 'צעד', 'מצוי', 'מעצבות', 'לשער', 'ידועה', 'נוהל', 'תם', 'מורד', 'ממונה', 'מאפיינים', 'מתמודד', 'לתחום', 'משקיף', 'קצרה', 'מבצעים', 'מוצעת', 'בנה', 'חשודים', 'מרגש', 'לבוש', 'משחק', 'לפותרה', 'קבע', 'עובדות', 'עסקה', 'חולה', 'העמידני', 'בלם', 'מונה', 'משותפים', 'חוזר', 'קשורה', 'הנחה', 'ידע', 'מוכר', 'מייצג', 'צפויים', 'נחת', 'לבטלן', 'תקנה', 'לזכות', 'לפצותו', 'באה', 'מדהימה', 'שוטף', 'הודו', 'כלה', 'יוצר', 'משלה', 'משחקים', 'שם', 'נוסף', 'כיוון', 'שרה', 'סוגר', 'לזהותם', 'חשש', 'מזכיר', 'שמע', 'רצח', 'לחלום', 'עולה', 'משדרים', 'עונה', 'פרשה', 'נזקקים', 'חתום', 'בדקה', 'לוחמים', 'פועלים', 'הצביעה', 'מהמר', 'מתקן', 'להחליפה', 'הפעיל', 'לבנות', 'נוהג', 'החכירה', 'הצעתי', 'קודמים', 'מיועדים', 'מרצים', 'השיגו', 'מקבל', 'מרכזים', 'לך', 'חיה', 'קדם', 'תובע', 'מצטיין', 'לזהותה', 'יושב', 'נס', 'נוסחה', 'היו', 'מפיק', 'לסדר', 'תומכים', 'כוללת', 'לשלב', 'לאחוז', 'בנוי', 'בדק', 'קודם', 'מדבר', 'נכונות', 'נוסע', 'בא', 'ללמדך', 'לפחד', 'להבינן', 'נחוץ', 'עבר', 'להספידו', 'עוינת', 'לחץ', 'ידועות', 'החל', 'מצליח', 'חדר', 'מתקדם', 'עקב', 'בולטים', 'מכנים', 'עמדה', 'סבורה', 'משתתף', 'מראה', 'קורא', 'מנה', 'פועל', 'ניתן', 'בוא', 'תמונה', 'פסק', 'צדק', 'ידועים', 'מעלה', 'הולם', 'חוקרים', 'מוגבלת', 'נטו', 'יתר', 'מאיר', 'להגדירה', 'מתנגדים', 'מורכבות', 'מוסר', 'נוסעים', 'לזמן', 'להוציאו', 'מפרנס', 'משתתפים', 'מונים', 'לייצגה', 'מפקח', 'קיים', 'מפתיעה', 'לחוללם', 'מבטיח', 'עומדת', 'הטרידו', 'מושכים', 'להזכירם', 'חשוד', 'הכתיבה', 'מקורבים', 'סחף', 'כובש', 'עדה', 'עובר', 'הורדה', 'מצא', 'הציבו', 'יורדים', 'סבור', 'גזר', 'לירות', 'לקוחה', 'משלים', 'מכר', 'מיידי', 'שירתו', 'גבו', 'הבין', 'מספיק', 'גובה', 'לחשוד', 'מסר', 'נגע', 'מחליף', 'מסיק', 'העז', 'קלע', 'דרך', 'רכש', 'אצבע', 'רצוי', 'סגר', 'תוקף', 'דובר', 'מודע', 'התאים', 'מקוצר', 'יוצאים', 'מקלות', 'שאלה', 'מעידה', 'לספר', 'כתב', 'הגישה', 'פצועים', 'קשורים', 'חשב', 'צורך', 'מבטא', 'דרוש', 'קונים', 'תלוי', 'יהיה', 'מצפה', 'שר', 'לעבר', 'לחלק', 'בוטל', 'לעודדם', 'ניצב', 'שלח', 'מסקר', 'נדרש', 'לאתר', 'לסגל', 'שבו', 'מוצעות', 'עמוסים', 'עבד', 'מתח', 'נושאים', 'עוזר', 'מבהיר', 'זעם', 'פסול', 'חלשה', 'לחסלה', 'חלקו', 'בהתחשב', 'החזיר', 'מחנה', 'יהיו', 'קשור', 'יצר', 'קיימת', 'אושר', 'התייחסו', 'מוביל', 'קוראים', 'כתבה', 'היתה', 'גבר', 'מובן', 'ערכו', 'לעניין', 'מאמצים', 'צופים', 'כולל', 'גרם', 'בוחרים', 'צבר', 'ישנה', 'מסופר', 'נהיה', 'חזה', 'לנצלם', 'הבטחת', 'דרושים', 'להעמידם', 'מרד', 'משרת', 'תומך', 'תרד', 'גדלים', 'לבדוק', 'לשנותן', 'יורשה', 'פתח', 'שכר', 'לאזור', 'פעילות', 'ארכה', 'ברור', 'מנהלת', 'מארס', 'סובל', 'להעלותו', 'הרבה', 'עולים', 'כותבים', 'סמך', 'דרכה', 'רווחה', 'בולט', 'פרק', 'לאמת', 'חוזרים', 'יבנה', 'נהג', 'כונס', 'רווח', 'ישוב', 'ממונים', 'פגע', 'יוצאת', 'מכוון', 'מתוכננים', 'אמור', 'מבוססות', 'מת', 'משכילים', 'פרוף', 'העיר', 'ברח', 'קיימים', 'יש', 'משותף', 'יקרה', 'כתובים', 'דיווח', 'מתאימה', 'מביכה', 'מתמחה', 'יוצא', 'סבל', 'מכבי', 'חברה', 'לנקום', 'מוכשרים', 'מסמלים', 'מורכבת', 'מזכירה', 'סמוך', 'יבוא', 'בנו', 'שבע', 'אמרה', 'תואר', 'חזרה', 'מאפיין', 'שבה', 'לצוות', 'מאורגנים', 'מדובר', 'פוגע', 'מעורב', 'עמידים', 'גורם', 'לחם', 'נתון', 'הלכה', 'בונה', 'חסם', 'מודאגת', 'מרשים', 'עסק', 'נשים', 'צעדו', 'יכולה', 'לערב', 'בטוח', 'מעודד', 'מונח', 'שלל', 'לראותו', 'ממשיכים', 'לזרזם', 'מכונה', 'כרך', 'לחקותו', 'חקר', 'קרבה', 'מטעה', 'פטורים', 'תהיה', 'נפרד', 'סבורים', 'מלווה', 'נכונים', 'להרחיקו', 'מנהלים', 'צפוי'}\n"
     ]
    }
   ],
   "source": [
    "verbs = {}\n",
    "non_verbs = {}\n",
    "\n",
    "for sentence in conllu:\n",
    "    for token in sentence:\n",
    "        if token.upos == 'VERB':\n",
    "            if token.form in verbs:\n",
    "                verbs.update({token.form : verbs[token.form]+1})\n",
    "            else:\n",
    "                verbs.update({token.form : 0})\n",
    "        else:\n",
    "            if token.form in non_verbs:\n",
    "                non_verbs.update({token.form : non_verbs[token.form]+1})\n",
    "            else:\n",
    "                non_verbs.update({token.form : 0})\n",
    "\n",
    "    \n",
    "print('{:,} unique verbs in training data'.format(len(verbs)))\n",
    "print('{:,} unique non-verbs in training data'.format(len(non_verbs)))\n",
    "\n",
    "ambiguous = set(verbs.keys()) & set(non_verbs.keys())\n",
    "\n",
    "print('{:,} words are ambiguous'.format(len(ambiguous)))\n",
    "print('ambiguous words:\\n' + str(ambiguous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T21:53:03.956006Z",
     "start_time": "2019-02-15T21:53:03.795667Z"
    }
   },
   "outputs": [],
   "source": [
    "characters = dict()\n",
    "\n",
    "lexicon = list(verbs.keys()) + list(non_verbs.keys())\n",
    "\n",
    "for verb in tqdm_notebook(lexicon):\n",
    "    for char in verb:\n",
    "        if char in characters:\n",
    "            characters[char] += 1\n",
    "        else:\n",
    "            characters[char] = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T21:53:47.753684Z",
     "start_time": "2019-02-15T21:53:47.745885Z"
    }
   },
   "outputs": [],
   "source": [
    "alphabet = characters.keys()\n",
    "alphabet = sorted(alphabet)\n",
    "print('the alhpabet size in this corpus is {}'.format(len(alphabet)))\n",
    "print('alphabet:\\n' + str(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T21:54:36.912522Z",
     "start_time": "2019-02-15T21:54:36.902323Z"
    }
   },
   "outputs": [],
   "source": [
    "letter_index = dict([(letter, idx) for idx, letter in enumerate(alphabet)]) # this is called a list comprehension\n",
    "\n",
    "def vectorize(word):\n",
    "    vec = [0] * len(alphabet)\n",
    "    for char in word:\n",
    "        vec[letter_index[char]] += 1\n",
    "    \n",
    "    return vec\n",
    "\n",
    "letter_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T22:27:04.192134Z",
     "start_time": "2019-02-15T22:27:04.183138Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, y, train_proportion=0.8):\n",
    "    ''' split the given data into train and test sets '''\n",
    "\n",
    "    assert len(X) == len(y), 'input data should have exactly one prediction per input'\n",
    "    assert 0 < train_proportion < 1, 'this function requires a proportion between zero and one as its second argument'\n",
    "    \n",
    "    data_indices = set(range(len(X)))\n",
    "    data_count = len(data_indices)\n",
    "\n",
    "    train_indices = set(random.sample(data_indices, int(data_count * train_proportion)))\n",
    "    test_indices  = data_indices - train_indices\n",
    "    \n",
    "    X_train = [X[idx] for idx in train_indices]\n",
    "    X_test  = [X[idx] for idx in test_indices]\n",
    "    \n",
    "    y_train = [y[idx] for idx in train_indices]\n",
    "    y_test  = [y[idx] for idx in test_indices]\n",
    "\n",
    "    assert len(X_train) + len(X_test) == len(X)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T22:41:05.236817Z",
     "start_time": "2019-02-15T22:41:05.082778Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_positive = list(map(vectorize, verbs.keys()))\n",
    "X_negative = list(map(vectorize, non_verbs.keys()))\n",
    "\n",
    "y_positive = [1] * len(X_positive)\n",
    "y_negative = [0] * len(X_negative)\n",
    "\n",
    "X_train_pos, X_test_pos, y_train_pos, y_test_pos = train_test_split(X_positive, y_positive)\n",
    "X_train_neg, X_test_neg, y_train_neg, y_test_neg = train_test_split(X_negative, y_negative)\n",
    "\n",
    "X_train = X_train_pos + X_train_neg\n",
    "y_train = y_train_pos + y_train_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T22:41:07.145614Z",
     "start_time": "2019-02-15T22:41:06.993943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T22:47:26.267711Z",
     "start_time": "2019-02-15T22:47:26.258436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': -9.913586387213542,\n",
       " '\"': -9.913586387213542,\n",
       " '%': -9.913586387213542,\n",
       " '(': -9.913586387213542,\n",
       " ')': -9.913586387213542,\n",
       " ',': -9.913586387213542,\n",
       " '-': -9.913586387213542,\n",
       " '.': -9.913586387213542,\n",
       " '0': -9.913586387213542,\n",
       " '1': -9.913586387213542,\n",
       " '2': -9.913586387213542,\n",
       " '3': -9.913586387213542,\n",
       " '4': -9.913586387213542,\n",
       " '5': -9.913586387213542,\n",
       " '6': -9.913586387213542,\n",
       " '7': -9.913586387213542,\n",
       " '8': -9.913586387213542,\n",
       " '9': -9.913586387213542,\n",
       " ':': -9.913586387213542,\n",
       " ';': -9.913586387213542,\n",
       " '?': -9.913586387213542,\n",
       " '_': -9.913586387213542,\n",
       " 'א': -3.8757154672914043,\n",
       " 'ב': -3.3525557213169686,\n",
       " 'ג': -4.000083381575272,\n",
       " 'ד': -3.727377763313048,\n",
       " 'ה': -2.5579452842392882,\n",
       " 'ו': -2.197571120570955,\n",
       " 'ז': -4.510909005341262,\n",
       " 'ח': -3.4259023687289316,\n",
       " 'ט': -4.282374605392176,\n",
       " 'י': -2.199801770614787,\n",
       " 'ך': -5.636920268197486,\n",
       " 'כ': -3.806563499471287,\n",
       " 'ל': -2.767601919499154,\n",
       " 'ם': -3.5767606560671004,\n",
       " 'מ': -2.704246130610632,\n",
       " 'ן': -5.093304821608505,\n",
       " 'נ': -3.045611978243249,\n",
       " 'ס': -3.8474782971097943,\n",
       " 'ע': -3.3696745416487497,\n",
       " 'ף': -5.5315597525396605,\n",
       " 'פ': -3.6183203857738953,\n",
       " 'ץ': -6.12939675329528,\n",
       " 'צ': -3.950007043595096,\n",
       " 'ק': -3.528391988215816,\n",
       " 'ר': -2.827684922847931,\n",
       " 'ש': -3.2128552776657315,\n",
       " 'ת': -2.655878710053499}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(letter_index.keys(), clf.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T22:51:55.820756Z",
     "start_time": "2019-02-15T22:51:55.813486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'ץץץץץץץץץץץץץץץץץץץץץץ'\n",
    "word = '12324983249898234'\n",
    "clf.predict([vectorize(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T23:24:12.901912Z",
     "start_time": "2019-02-15T23:24:12.836836Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = X_test_pos + X_test_neg\n",
    "y_test = y_test_pos + y_test_neg\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T23:29:20.907984Z",
     "start_time": "2019-02-15T23:29:20.895829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      1.00      0.91      5046\n",
      "          1       0.00      0.00      0.00       997\n",
      "\n",
      "avg / total       0.70      0.83      0.76      6043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T23:29:24.899427Z",
     "start_time": "2019-02-15T23:29:24.880214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5042,    4],\n",
       "       [ 997,    0]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
